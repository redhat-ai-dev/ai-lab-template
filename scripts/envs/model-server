# all default common env var values are defined in scripts/envs/base file
# in order to override a default common env var value and/or add a custom
# env var used only for this sample you can list it here
export APP_NAME="model-server"
export APP_DISPLAY_NAME="Model Server, No Application"
export APP_DESC="Deploy a granite-3.1 8b model with a vLLM server. While no application is configured, this model server can be utilized in other Software Templates, like a Chatbot Application for instance."
export APP_SUMMARY="A granite-3.1 8b model server deployment."
export APP_TAGS='["ai", "vllm", "modelserver"]'

# https://github.com/redhat-ai-dev/developer-images/tree/main/model-servers/vllm/0.8.4
# export VLLM_CONTAINER="quay.io/redhat-ai-dev/vllm-openai-ubi9:v0.8.4"
export VLLM_CONTAINER="quay.io/rh-ee-lyoon/vllm-openai-ubi9:v0.11.0"
export VLLM_DESC="A high throughput, memory efficient inference and serving engine with GPU support for LLMs in OpenShift"
export VLLM_SRC="https://github.com/redhat-ai-dev/developer-images/tree/main/model-servers/vllm/0.8.4"

# https://huggingface.co/ibm-granite/granite-3.1-8b-instruct
export LLM_MODEL_NAME="ibm-granite/granite-3.1-8b-instruct"
export LLM_MAX_MODEL_LEN=4096
export LLM_MODEL_CLASSIFICATION="Text Generation"
export LLM_MODEL_LICENSE="Apache-2.0"
export LLM_MODEL_SRC="https://huggingface.co/ibm-granite/granite-3.1-8b-instruct"

#model configuration
export SUPPORT_LLM=true
export SUPPORT_LLM_LLAMA=false

# model server configuration
export SUPPORT_EXISTING=false

# application configuration
export SUPPORT_APP=false
export CLEANUP_APP=true