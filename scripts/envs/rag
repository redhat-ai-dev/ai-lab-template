# all default common env var values are defined in scripts/envs/base file
# in order to override a default common env var value and/or add a custom
# env var used only for this sample you can list it here
export APP_NAME="rag"
export APP_DISPLAY_NAME="RAG Chatbot Application"
export APP_DESC="Enhance the capabilities of your Chatbot using a Retrieval-Augmented Generation (RAG) application. Pick from the model servers available or bring your own."
export APP_SUMMARY="RAG (Retrieval Augmented Generation) streamlit chat application with a vector database. Upload a file containing relevant information to train the model for more accurate responses."
export APP_TAGS='["ai", "llamacpp", "vllm", "python", "rag", "database"]'
export APP_RUN_COMMAND="streamlit run rag_app.py"

# https://github.com/redhat-ai-dev/developer-images/tree/main/models/granite-3.1-8b-instruct-gguf
export INIT_CONTAINER="quay.io/redhat-ai-dev/granite-3.1-8b-instruct-gguf:latest"

# https://github.com/redhat-ai-dev/developer-images/tree/main/model-servers/llamacpp_python/0.3.16
export MODEL_SERVICE_CONTAINER="quay.io/redhat-ai-dev/llamacpp_python:0.3.16"
export MODEL_SERVICE_DESC="A Python binding of LLM inference in C/C++ with minimal setup"
export MODEL_SERVICE_SRC="https://github.com/redhat-ai-dev/developer-images/tree/main/model-servers/llamacpp_python/0.3.16"

# https://github.com/redhat-ai-dev/developer-images/tree/main/model-servers/vllm/0.11.0
export VLLM_CONTAINER="quay.io/redhat-ai-dev/vllm-openai-ubi9:v0.11.0"
export VLLM_DESC="A high throughput, memory efficient inference and serving engine with GPU support for LLMs in OpenShift"
export VLLM_SRC="https://github.com/redhat-ai-dev/developer-images/tree/main/model-servers/vllm/0.11.0"

# https://huggingface.co/ibm-granite/granite-3.1-8b-instruct
export LLM_MODEL_NAME="ibm-granite/granite-3.1-8b-instruct"
export LLM_MAX_MODEL_LEN=4096
export LLM_MODEL_CLASSIFICATION="Text Generation"
export LLM_MODEL_LICENSE="Apache-2.0"
export LLM_MODEL_SRC="https://huggingface.co/ibm-granite/granite-3.1-8b-instruct"

# model configurations
export SUPPORT_LLM=true

# for gitlab case, since gitlab does not have pipeline webhook pre-set to trigger the initial build
export APP_INTERFACE_CONTAINER="quay.io/redhat-ai-dev/rag:latest"

# Database configurations
export SUPPORT_DB=true
export CLEANUP_DB=false
export DB_CONTAINER="quay.io/redhat-ai-dev/chroma:latest"
export DB_PORT=8000

export TEMPLATE_SOURCE_URL="https://github.com/redhat-ai-dev/ai-lab-samples/tree/main/rag"

# application configuration
# Reference the above info as different names for templating purposes.
# Should probably be refactored.
export MODEL_NAME="${LLM_MODEL_NAME}"
export MODEL_SRC="${LLM_MODEL_SRC}"
export MODEL_SERVICE_SRC_OTHER="${MODEL_SERVICE_SRC}"
export MODEL_SERVICE_SRC_VLLM="${VLLM_SRC}"